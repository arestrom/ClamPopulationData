#=======================================================================================
# Initial test of clam pop data
#
# Steps:
#  1. 
#
#  ToDo:
#  1.
# 
#  Notes: 
#  1. The gdb files in 2022 GPS and Field Notes do not download using sf in R
#     But the gpx files open correctly. The gdb files do open in QGIS and match
#     the gpx files exactly for WPennCove 2022. 
#  2. Doug confirmed any and all edits to pop data will be in the BeachEdits folder
#     (ie, 2022.240150.00Bch.xlsx) for location and basic transect data. So use the 
#     BeachEdits data as baseline. Compare beach edits to gpx and gdb files.
#  3. Doug confirmed that he would never go back to find missing samples. Very rare
#     occurrance was Karla way back when. It is ok to interpolate positions and times
#     for cases where sample location and times are missing.
#  4. Doug confirmed that tallyclam and wet lab data to not overlap. For cases where
#     all sample numbers were brought back to wet lab, there would be no tally clam 
#     data for the beach (ie. Point Whitney, 2022). In 2020 there were basically no
#     wet lab data due to COVID. 
#  5. Doug confirmed that for wet lab data the correct data to use are in the Safe Copies
#     folder. 
#  6. Can calculate lead distance with code from:
#     https://stackoverflow.com/questions/49853696/distances-of-points-between-rows-with-sf
#  7. There were incomplete data in the Access DBs for TallyClam in the 2022 Clam Tally Data
#     folder. The comprehensive Access DB at: Clam&OysterPopSurveyData looks to be complete.
#
# AS 2023-05-29
#=======================================================================================

# Clear workspace
rm(list = ls(all.names = TRUE))

# Load libraries
library(odbc)
library(RODBC)
library(DBI)
library(RPostgres)
library(dplyr)
library(uuid)
library(tidyr)
library(sf)
library(stringi)
library(lubridate)
library(glue)
library(openxlsx)
library(leaflet)
library(fs)
library(tools)
library(terra)

# Set options
options(digits=15)

# Keep connections pane from opening
options("connectionObserver" = NULL)

#=====================================================================================
# Function to get user for database
pg_user <- function(user_label) {
  Sys.getenv(user_label)
}

# Function to get pw for database
pg_pw <- function(pwd_label) {
  Sys.getenv(pwd_label)
}

# Function to get pw for database
pg_host <- function(host_label) {
  Sys.getenv(host_label)
}

# Function to connect to postgres
pg_con_local = function(dbname, port = '5432') {
  con <- dbConnect(
    RPostgres::Postgres(),
    host = pg_host("pg_host_local"),
    dbname = dbname,
    user = pg_user("pg_user"),
    password = pg_pw("pg_pwd_local"),
    port = port)
  con
}

# Generate a vector of Version 4 UUIDs (RFC 4122)
get_uuid = function(n = 1L) {
  if (!typeof(n) %in% c("double", "integer") ) {
    stop("n must be an integer or double")
  }
  uuid::UUIDgenerate(use.time = FALSE, n = n)
}

#============================================================================
# Import all data from survey and encounters
# For now am not syncing. Just copy over to local folder and grab as below
# The autogenerated names for layers seem to be set. Do not match survey id
# or encounter_id in data. 
#============================================================================

# Define current_year
current_year = 2022

# Define path to GPS data
gps_path = "data\\2022\\2022 GPS and Field Notes\\CLAM\\GPS Downloaded Maps\\"
beach_edits_path = "data\\2022\\2022 GPS and Field Notes\\CLAM\\Beach Edits\\"
beach_number_path = "data\\2022\\2022 Clam\\2022 Clam Tally Data\\2022.09.14TallyClam.mdb"
tally_path = "data\\Clam&OysterPopSurveyData\\CLAM\\ClamTally2002-2022.accdb"
wet_lab_path = "data\\Clam&OysterPopSurveyData\\CLAM\\ClamPopLengthWeight.accdb"

# Check extensions that occur in the pictures folder
gps_files = dir_ls(gps_path, recurse = TRUE)
unique(tools::file_ext(gps_files))

# Make vector of all the gpx files
gpx_paths = subset(gps_files, tools::file_ext(gps_files) %in% c("gpx"))

# Inspect
# head(gpx_paths, 20)
(gpx_files = basename(gpx_paths))

# Make vector of all beach edits xlsx files
beach_edit_files = dir_ls(beach_edits_path, recurse = TRUE)
unique(tools::file_ext(beach_edit_files))

# Make vector of all xlsx files in the folder
xlsx_paths = subset(beach_edit_files, tools::file_ext(beach_edit_files) %in% c("xlsx"))

# Inspect
(xlsx_files = basename(xlsx_paths))

# Get the bidn names
con = odbcConnectAccess2007(beach_number_path)
beach_id = RODBC::sqlQuery(con, as.is = TRUE, "SELECT * FROM BIDN")
close(con) 

# Get the clam tally data. The only complete source in 2022
con = odbcConnectAccess2007(tally_path)
legals = RODBC::sqlQuery(con, as.is = TRUE, "SELECT * FROM Legals")
close(con) 

# Get the wet lab data. Assume it's complete. Use WetLabData SafeCopies xlsx files if not. 
con = odbcConnectAccess2007(wet_lab_path)
wet_lab = RODBC::sqlQuery(con, as.is = TRUE, "SELECT * FROM LengthWeight")
close(con) 

# Organize the legals data
legals = legals |> 
  mutate(Date = substr(Date, 1, 10)) |> 
  select(bidn = BIDN, date_part = Date, sample_number = `Sample Number`,
         manila = Manila, native = Native, butter = Butter, cockle = Cockle,
         eastern = Eastern, horse = Horse, varnish = Varnish)

# Organize the wet_lab data
wet_lab = wet_lab |> 
  mutate(BIDN = as.integer(BIDN)) |> 
  select(BIDN, year = Year, sample_number = Sample_Number, species = Species,
         count = Count, length = Length, weight = Weight, flag = Flag)

# Get location IDs for the current year beaches
con = pg_con_local("ps_shellfish")
qry = glue("SELECT loc.location_id, lb.boundary_code, lb.boundary_name, ",
           "lb.active_datetime ",
           "FROM location as loc ",
           "left join location_type_lut as lt ",
           "on loc.location_type_id = lt.location_type_id ",
           "left join location_boundary as lb on ",
           "loc.location_id = lb.location_id ",
           "where lt.location_type_description = 'Beach polygon, Intertidal management' ",
           "and date_part('year', active_datetime) = {current_year} ",
           "order by active_datetime desc")
beach_info = DBI::dbGetQuery(con, qry)
DBI::dbDisconnect(con)

# Check for duplicate BIDNs
if ( any(duplicated(beach_info$boundary_code)) ) {
  cat("\nAt least one duplicated BIDN. Investigate!\n\n") 
} else {
  cat("\nNo duplicated BIDNs. Ok to proceed.\n\n")
}

# Check for duplicate location IDs
if ( any(duplicated(beach_info$location_id)) ) {
  cat("\nAt least one duplicated location ID. Investigate!\n\n") 
} else {
  cat("\nNo duplicated location IDs. Ok to proceed.\n\n")
}

# Check for duplicate beach names
if ( any(duplicated(beach_info$boundary_name)) ) {
  cat("\nAt least one duplicated beach name. Investigate!\n\n") 
} else {
  cat("\nNo duplicated beach names. Ok to proceed.\n\n")
}

#==============================================================================================
# First Beach
#==============================================================================================

# Define the increment
i = 1

# Define the bidn
beach_number = substr(gpx_files[i], 6, 14)

# Get data from gpx and xlsx to compare.
dat_gpx = st_read(gpx_paths[i], layer = "waypoints") |> 
  mutate(latitude = as.numeric(st_coordinates(geometry)[,2])) |> 
  mutate(longitude = as.numeric(st_coordinates(geometry)[,1])) |> 
  mutate(BIDN = beach_number) |> 
  mutate(waypoint = as.integer(name)) |> 
  left_join(beach_id, by = "BIDN") |> 
  select(waypoint, bidn = BIDN, beach_name = Name, time, latitude, longitude, elevation = ele)

# Check crs
st_crs(dat_gpx)$epsg

# # Get data from gdb to compare. Will not open. Need to use QGIS
# wpenncoveb = terra::vect(glue(gps_path, "2022.240150.00.gdb"), layer = "waypoints")
# wpenncoveb = sf::st_read(glue(gps_path, "2022.240150.00.gdb"), layer = "waypoints")

# Read xlsx
dat_xlsx = read.xlsx(xlsx_paths[i], detectDates = TRUE) |> 
  mutate(time = openxlsx::convertToDateTime(Description)) |> 
  mutate(BIDN = beach_number) |> 
  mutate(Waypoint = as.integer(Waypoint)) |> 
  mutate(geometry = mapply(function(x,y) st_point(c(x,y)), 
                           Longitude, Latitude,SIMPLIFY = FALSE)) |> 
  mutate(geometry = st_as_sfc(geometry, crs = 4326)) |> 
  filter(!X16 %in% c("sqft", "acres")) |>
  left_join(beach_id, by = "BIDN") |> 
  mutate(sample_number = as.integer(SAMPNO)) |> 
  select(waypoint = Waypoint, bidn = BIDN, beach_name = Name, time,
         latitude = Latitude, longitude = Longitude, surveyor = SURVEYOR,
         pace = PACE, waypoint_type = TYPE, sample_number, 
         transect = TRANSECT, transpace = TRANSPAC,
         shorepace = SHOREPAC, geometry)
  
# Join to compare datetimes and coordinates
nrow(dat_gpx); nrow(dat_xlsx)
nrow(dat_gpx) == nrow(dat_xlsx)
chk_dat = dat_gpx |> 
  select(waypoint, time_gpx = time, lat = latitude, lon = longitude) |> 
  left_join(dat_xlsx, by = "waypoint") |> 
  mutate(time_chk = time_gpx - time) |>
  mutate(dist_chk = st_distance(geometry.x, geometry.y, by_element = TRUE)) |> 
  mutate(dist_m = round(dist_chk, digits = 4)) |> 
  st_drop_geometry() |> 
  select(waypoint, time_gpx, time, time_chk, dist_m)

# # Add some error to test code below
# chk_dat$time_chk[1] = chk_dat$time_chk[1] + 2
# chk_dat$dist_m[2] = chk_dat$dist_m[2] + units::set_units(2, m)

# Pull out rows that differ in time or location
chk_dat = chk_dat |> 
  filter(is.na(time_chk) | 
           time_chk > as.difftime("00:00:00") | 
           is.na(dist_m) | 
           dist_m > units::set_units(1, m))

# Result: Use xlsx. Two rows deleted. Sample numbers are consecutive in xlsx across deletions
#         But add elevation from gpx file
gpx_elev = dat_gpx |> 
  st_drop_geometry() |> 
  select(waypoint, elevation)

dat_xlsx = dat_xlsx |> 
  left_join(gpx_elev, by = "waypoint") |> 
  select(waypoint, bidn, beach_name, time, latitude, longitude, elevation, 
         surveyor, pace, waypoint_type,sample_number, transect, transpace, 
         shorepace, geometry)

# Filter out tally data for current beach and year
tally_dat = legals |> 
  filter(bidn == as.numeric(beach_number) & substr(date_part, 1, 4) == "2022") |> 
  select(beach_number = bidn, date_part, sample_number, manila, native, butter,
         cockle, eastern, horse, varnish)

# Combine tally data with base survey data
dat = dat_xlsx |> 
  mutate(sample_number = as.numeric(sample_number)) |> 
  left_join(tally_dat, by = "sample_number")

# Sanity check
if ( nrow(dat) != nrow(dat_xlsx) ) {
  cat("\nIncorrect number of rows in dat. Investigate!\n\n")
} else {
  cat("\nNumber of rows correct. Ok to proceed\n\n")
}
  
#==========================================================
# Add summary info from wet lab. Verify all samples present
#==========================================================

# Pull out current_year and beach wet_lab data
wet_dat = wet_lab |> 
  filter(BIDN == as.integer(beach_number) & year == as.numeric(current_year)) |> 
  select(BIDN, year_part = year, sample_number, species, count, length, weight,
         flag)

# Inspect BIDN and year entries
unique(wet_dat$BIDN)
unique(wet_dat$year)

# Create a summary dataset with sample_number and counts for matching with dat
wet_sums = wet_dat |> 
  group_by(sample_number) |> 
  summarise(wet_count = sum(count, na.rm = TRUE)) |> 
  ungroup() |> 
  select(sample_number, wet_count)

# Check for duplicated sample_numbers
any(duplicated(wet_sums$sample_number))

# Check if there are any sample numbers in wet_sums that are not in dat
# Should be none
wet_sums$sample_number[!wet_sums$sample_number %in% dat$sample_number]
# And the reverse...should be several
# dat$sample_number[!dat$sample_number %in% wet_sums$sample_number]

# Add wet_sums to dat
dat = dat |> 
  left_join(wet_sums, by = "sample_number") 

# Check that all sample numbers have a count associated
cols = c("manila", "native", "butter", "cockle", "eastern", "horse", "varnish")
chk_dat = dat |> 
  st_drop_geometry() |> 
  mutate(tally_count = rowSums(across(all_of(cols)))) |> 
  select(bidn, beach_number, beach_name, time, sample_number, wet_count, tally_count)

# Filter to see if any cases with sample_number have neither a wet count or tally count
chk_dat2 = chk_dat |> 
  filter(!is.na(sample_number) & is.na(wet_count) & is.na(tally_count))

# Sanity check
if ( nrow(chk_dat2) > 0L ) {
  cat("\nSome sample_numbers unnaccounted for. Investigate!\n\n")
} else {
  cat("\nAll sample numbers accounted for. Ok to proceed\n\n")
}

#==========================================================
# Generate survey level data
#==========================================================

# Check geometry in dat
st_crs(dat$geometry)$epsg

# Pull out survey level data
srv = dat |> 
  st_drop_geometry() |> 
  select(bidn, time, surveyor) |> 
  distinct() |> 
  mutate(survey_datetime = with_tz(min(time), "UTC")) |> 
  mutate(start_datetime = with_tz(min(time), "UTC")) |> 
  mutate(end_datetime = with_tz(max(time), "UTC"))

# Start trimming and adding as needed
survey = srv |> 
  select(bidn, survey_datetime, start_datetime, end_datetime, surveyor) |> 
  distinct()

# There must be only one row...Verify!
if ( nrow(survey) > 1L ) {
  cat("\nThere must be only one row. Investigate!\n\n")
} else {
  cat("\nOnly one row. Ok to proceed!\n\n")
}

# Add boundary ID 
beach_info = beach_info |> 
  mutate(bidn = paste0(boundary_code, ".00")) |> 
  select(location_id, boundary_code, bidn, boundary_name, active_datetime)

# Add info
survey = survey |> 
  left_join(beach_info, by = "bidn") |> 
  mutate(survey_id = get_uuid(1L)) |> 
  mutate(survey_type_id = "4bfd73c6-79dd-4428-a2b7-d8f1aa2326a4") |>               # Clam Population Survey
  mutate(organizational_unit_id = "f1de1d54-d750-449f-a700-dc33ebec04c6") |>       # Intertidal Mngmnt Unit
  mutate(area_surveyed_id = "4e072477-e5f1-4a03-a14c-18e4aeade76e") |>             # Not applicable. Have coords.
  mutate(data_review_status_id = "59cbf9d7-9015-49b6-874d-ea37ef663c56") |>        # Reviewed
  mutate(survey_completion_status_id = "d192b32e-0e4f-4719-9c9c-dec6593b1977") |>  # Completed survey
  mutate(comment_text =                                                            # From PopWork.xlsx
           "Four first time clam samplers (GWH, Stilliguamish Tribe, WCC)") |> 
  mutate(created_datetime = with_tz(Sys.time(), "UTC")) |> 
  #mutate(created_by = Sys.getenv("USERNAME")) |> 
  mutate(created_by = "stromas") |>                                                # Don't use in future !
  mutate(modified_datetime = with_tz(as.POSIXct(NA), "UTC")) |> 
  mutate(modified_by = NA_character_) |> 
  select(survey_id, survey_type_id, organizational_unit_id, location_id,
         area_surveyed_id, data_review_status_id, survey_completion_status_id,
         survey_datetime, start_datetime, end_datetime, comment_text, 
         created_datetime, created_by, modified_datetime, modified_by)
  
# Generate data for person table (ONLY IF NOT ALREADY PRESENT!!!!!!!!!!!!!!!!)
person = tibble::tibble(
  person_id = get_uuid(2L),
  first_name = c("Camille", "Doug"),
  last_name = c("Speck", "Rogers"),
  active_indicator = rep(1L, 2),
  created_datetime = rep(with_tz(Sys.time(), "UTC"), 2),
  created_by = rep("stromas", 2),                              # Do not use in future !!!!!
  #created_by = rep(Sys.getenv("USERNAME"), 2),                # Use this line instead !!!!!
  modified_datetime = rep(with_tz(as.POSIXct(NA), "UTC"), 2),
  modified_by = rep(NA_character_, 2))

# Generate data for survey_person table...PULL FROM DB FOR SUBSEQUENT CASES !!!!!!!!!!!!!
survey_person = tibble::tibble(
  survey_person_id = get_uuid(1L),
  survey_id = survey$survey_id,
  person_id = person$person_id[1])        # Edit here for use in future !!!!!!

# Any duplicated waypoints?
any(duplicated(dat$waypoint))

# Generate survey_event data. Make sure waypoint is not duplicated. It is used for join later
srv_event = dat |> 
  mutate(survey_id = survey$survey_id) |> 
  mutate(event_number = as.integer(waypoint)) |> 
  mutate(event_datetime = with_tz(time, "UTC")) |> 
  select(survey_id, event_number, event_datetime) |> 
  distinct()

# Add needed columns
srv_event = srv_event |> 
  mutate(survey_event_id = get_uuid(nrow(srv_event))) |> 
  mutate(event_location_id = get_uuid(nrow(srv_event))) |> 
  mutate(harvester_type_id = "df751c90-3f80-4a3c-b67c-c496dd2201b9") |>       # Not applicable
  mutate(harvest_method_id = "68cb2cb1-77df-49f3-872b-5e3ba3299e14") |>       # Not applicable
  mutate(harvest_gear_type_id = "5d1e6be6-cd85-498c-b2fa-43b370d951b4") |>    # Not applicable
  mutate(harvest_depth_range_id = "b27281f7-9387-4a51-b91f-95748676f918") |>  # Not applicable
  mutate(harvester_count = NA_integer_) |> 
  mutate(harvest_gear_count = NA_integer_) |> 
  mutate(harvester_zip_code = NA_character_) |> 
  mutate(comment_text = NA_character_) |> 
  mutate(created_datetime = with_tz(Sys.time(), "UTC")) |> 
  #mutate(created_by = Sys.getenv("USERNAME")) |> 
  mutate(created_by = "stromas") |>                                                # Don't use in future !
  mutate(modified_datetime = with_tz(as.POSIXct(NA), "UTC")) |> 
  mutate(modified_by = NA_character_) |> 
  st_drop_geometry() |> 
  select(survey_event_id, survey_id, event_location_id, harvester_type_id,
         harvest_method_id, harvest_gear_type_id, harvest_depth_range_id,
         event_number, event_datetime, harvester_count, harvest_gear_count,
         harvester_zip_code, comment_text, created_datetime, created_by, 
         modified_datetime, modified_by)

# Get dataset so survey_event_id can be joined to dat via waypoint
sev_ids = srv_event |> 
  select(survey_event_id, location_id = event_location_id, waypoint = event_number)

# Check the categories of waypoint_type
unique(dat$waypoint_type)
any(is.na(dat$waypoint_type))

# Generate location and location_coordinates data for each waypoint (event_number)
surv_loc = dat |> 
  left_join(sev_ids, by = "waypoint") |> 
  mutate(location_type_id = case_when(
    waypoint_type == "Bndy-btm" ~ "5bc29f85-ee83-4ddb-886d-2ffeccb776ca",
    waypoint_type == "Bndy-top" ~ "bbdf1e05-3441-458a-9ba8-b9df97c5438e",
    waypoint_type == "Top" ~ "704cd000-e8fc-4a30-a030-7b404e4d6904",
    waypoint_type == "Bottom" ~ "f05c7866-c630-4309-8a32-b9d599a8b505",
    waypoint_type == "Sample" ~ "d96f1f74-699c-496b-a090-565b489a4738",
    is.na(waypoint_type) ~ NA_character_)) |>          
  mutate(location_code = as.character(sample_number)) |> 
  mutate(location_name = NA_character_) |> 
  mutate(location_description = NA_character_) |> 
  mutate(comment_text = NA_character_) |> 
  select(location_id, location_type_id, location_code, location_name,
         location_description, comment_text, geometry)

# Pull out just location table data
location = surv_loc |> 
  st_drop_geometry() |>
  mutate(created_datetime = with_tz(Sys.time(), "UTC")) |> 
  #mutate(created_by = Sys.getenv("USERNAME")) |> 
  mutate(created_by = "stromas") |>   # Don't use in future !
  mutate(modified_datetime = with_tz(as.POSIXct(NA), "UTC")) |> 
  mutate(modified_by = NA_character_) |> 
  select(location_id, location_type_id, location_code, location_name,
         location_description, comment_text, created_datetime, 
         created_by, modified_datetime, modified_by)
  
# Pull out location_coordinates data. Check epsg
location_coordinates = surv_loc |> 
  select(location_id, geog = geometry)
  
# Check geography in location_coordinates
location_coordinates = st_as_sf(location_coordinates)
st_crs(location_coordinates)$epsg

# Add missing columns. 
# No need for gid. Will use code from "copy_current_shellfish_to_ps_shellfish.R"
# This uses INSERT INTO syntax copying from a temp table in same database
location_coordinates = location_coordinates |> 
  mutate(location_coordinates_id = get_uuid(nrow(location_coordinates))) |> 
  mutate(horizontal_accuracy = NA_real_) |> 
  mutate(created_datetime = with_tz(Sys.time(), "UTC")) |> 
  #mutate(created_by = Sys.getenv("USERNAME")) |> 
  mutate(created_by = "stromas") |>       # Don't use in future !
  mutate(modified_datetime = with_tz(as.POSIXct(NA), "UTC")) |> 
  mutate(modified_by = NA_character_) |> 
  select(location_coordinates_id, location_id, horizontal_accuracy,
         geog, created_datetime, created_by, modified_datetime,
         modified_by)

# Get species_encounter data
spec_enc = dat |> 
  st_drop_geometry() |> 
  left_join(sev_ids, by = "waypoint") |> 
  filter(!is.na(sample_number)) |> 
  select(survey_event_id, sample_number, wet_count, manila, native,
         butter, cockle, eastern, horse, varnish)

# Pull out each category of tally count species, then stack into one dataset
manila = spec_enc |> 
  select(survey_event_id, species_count = manila, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "a19761a6-f1f8-486f-af1f-94e129fbfb62") |> 
  mutate(species = "manila") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Pull out each category of tally count species, then stack into one dataset
native = spec_enc |> 
  select(survey_event_id, species_count = native, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "906931e9-b480-44c9-b798-139bbaee4630") |> 
  mutate(species = "native") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Pull out each category of tally count species, then stack into one dataset
butter = spec_enc |> 
  select(survey_event_id, species_count = butter, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "5c92ce13-de44-44f7-b51d-c1e50fa9ec99") |> 
  mutate(species = "butter") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Pull out each category of tally count species, then stack into one dataset
cockle = spec_enc |> 
  select(survey_event_id, species_count = cockle, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "c4f1b299-b46a-46db-b9b4-1074744b0a07") |> 
  mutate(species = "cockle") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Pull out each category of tally count species, then stack into one dataset
eastern = spec_enc |> 
  select(survey_event_id, species_count = eastern, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "4fb72cc7-a4c3-4cf4-acde-32bea9f15a43") |> 
  mutate(species = "eastern") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Pull out each category of tally count species, then stack into one dataset
horse = spec_enc |> 
  select(survey_event_id, species_count = horse, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "383ca6a6-aeb9-4b67-af31-4007f3ce0dc1") |> 
  mutate(species = "horse") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Pull out each category of tally count species, then stack into one dataset
varnish = spec_enc |> 
  select(survey_event_id, species_count = varnish, sample_number) |> 
  filter(!is.na(species_count)) |> 
  mutate(species_id = "821c6569-8631-494a-926a-560a81075b14") |> 
  mutate(species = "varnish") |> 
  select(survey_event_id, species_id, species, sample_number, species_count)

# Stack
tally_clams = rbind(manila, native, butter, cockle, eastern, horse, varnish) |> 
  arrange(sample_number)

# Notes:
#  1. We do not need pace, transpace, or shorepace since we have coordinates
#  2. We can create a transect in the route table and name each transect by the transect number, if needed
#  3. Sample numbers are being added to location table as location_code.
#  4. Each wet-lab species has a count of one and needs to be added one line at a time to species_encounter
#     since that's where we enter the species_count value and shell condition.
#  5. Enter the length and weight to the individual_species table. 
#  6. Macoma's are given lumped weights where more than one found in a sample. Should enter all lumped counts
#     to species_encounter table. There are no lengths for macomas. Weights for lumped counts of macomas should 
#     be added to the species_encounter table. There is no advantage to adding weights for single macomas to 
#     the individual_species table, so for consistency, add to the species_encounter table.
#  7. Need to know what LO stands for in flag column.

# Verify the wet_lab data all have sample numbers in beach_edit data
all(unique(wet_dat$sample_number) %in% unique(spec_enc$sample_number))
unique(wet_dat$year_part)
unique(wet_dat$BIDN)
unique(wet_dat$species)
  
# Generate wet_lab data for the species_encounter table. 
# Includes data for individual_species table
lab_clams = wet_dat |> 
  left_join(spec_enc, by = "sample_number") |> 
  select(survey_event_id, species, sample_number, species_count = count,
         wet_count, length, weight, flag)

# Check the flag codes
unique(lab_clams$flag)

# LO = length only; OK = Both length and weight; WO = Weight only; 
# BL = Broken Legal; BS = Broken Sub-legal; MT = Empty; FL = Flooded;




#==============================================================================================
# Next Beach
#==============================================================================================

# Compile a final dataset
master_beach_edit = dat_xlsx

# Define the increment
i = 2

# Define the bidn
bidn = substr(gpx_files[i], 6, 14)

# Get data from gpx and xlsx to compare.
dat_gpx = st_read(gpx_paths[i], layer = "waypoints") |> 
  mutate(latitude = as.numeric(st_coordinates(geometry)[,2])) |> 
  mutate(longitude = as.numeric(st_coordinates(geometry)[,1])) |> 
  mutate(BIDN = bidn) |> 
  mutate(waypoint = as.integer(name)) |> 
  left_join(beach_id, by = "BIDN") |> 
  select(waypoint, bidn = BIDN, beach_name = Name, time, latitude, longitude, elevation = ele)

# Check crs
st_crs(dat_gpx)$epsg

# Read xlsx
dat_xlsx = read.xlsx(xlsx_paths[i], detectDates = TRUE) |> 
  # NEED OTHER CODE HERE !!!!!!!!!!!!
  mutate(time = openxlsx::convertToDateTime(Description)) |> 
  mutate(BIDN = bidn) |> 
  mutate(Waypoint = as.integer(Waypoint)) |> 
  mutate(geometry = mapply(function(x,y) st_point(c(x,y)), 
                           Longitude, Latitude,SIMPLIFY = FALSE)) |> 
  mutate(geometry = st_as_sfc(geometry, crs = 4326)) |> 
  filter(!X16 %in% c("sqft", "acres")) |>
  left_join(beach_id, by = "BIDN") |> 
  mutate(sample_number = as.integer(SAMPNO)) |> 
  select(waypoint = Waypoint, bidn = BIDN, beach_name = Name, time,
         latitude = Latitude, longitude = Longitude, surveyor = SURVEYOR,
         pace = PACE, sample_number, transect = TRANSECT, transpace = TRANSPAC,
         shorepace = SHOREPAC, geometry)

# Join to compare datetimes and coordinates
nrow(dat_gpx); nrow(dat_xlsx)
nrow(dat_gpx) == nrow(dat_xlsx)
chk_dat = dat_gpx |> 
  select(waypoint, time_gpx = time, lat = latitude, lon = longitude) |> 
  left_join(dat_xlsx, by = "waypoint") |> 
  mutate(time_chk = time_gpx - time) |>
  mutate(dist_chk = st_distance(geometry.x, geometry.y, by_element = TRUE)) |> 
  mutate(dist_m = round(dist_chk, digits = 4)) |> 
  st_drop_geometry() |> 
  select(waypoint, time_gpx, time, time_chk, dist_m)

# # Add some error to test code below
# chk_dat$time_chk[1] = chk_dat$time_chk[1] + 2
# chk_dat$dist_m[2] = chk_dat$dist_m[2] + units::set_units(2, m)

# Pull out rows that differ in time or location
chk_dat = chk_dat |> 
  filter(is.na(time_chk) | 
           time_chk > as.difftime("00:00:00") | 
           is.na(dist_m) | 
           dist_m > units::set_units(1, m))

# Result: Use xlsx. Two rows deleted. Sample numbers are consequetive in xlsx across deletions
#         But add elevation from gpx file
gpx_elev = dat_gpx |> 
  st_drop_geometry() |> 
  select(waypoint, elevation)

dat_xlsx = dat_xlsx |> 
  left_join(gpx_elev, by = "waypoint") |> 
  select(waypoint, bidn, beach_name, time, latitude, longitude, elevation, 
         surveyor, pace, sample_number, transect, transpace, shorepace, 
         geometry)

# Compile a final dataset
master_beach_edit = dat_xlsx













current_layers$name
encounters_layer = current_layers$name[1]
survey_layer = current_layers$name[2]
track_layer = current_layers$name[3]
person_layer = current_layers$name[4]
fungi_layer = current_layers$name[5]
plants_layer = current_layers$name[6]
trees_layer = current_layers$name[7]

# Get all fields from survey
survey = st_read(glue(qfield_path, "data.gpkg"), layer = survey_layer)
survey = survey |> 
  select(person = companions, survey_comments, survey_id, start_point_name,
         start_time, end_time, target_species, created_time, created_by,
         form_name, form_version, survey_timestamp, survey_horiz_accuracy,
         survey_source, survey_gnss_timestamp, survey_gnss_horiz_accuracy, 
         geom)

# Get all fields from encounters
encounters = st_read(glue(qfield_path, "data.gpkg"), layer = encounters_layer)

# Get all fields from person (previously companions)
new_person = st_read(glue(qfield_path, "data.gpkg"), layer = person_layer)
new_person = new_person |> 
  select(fid, person_id = companions_id, first_name,
         last_name, full_name, created_datetime,
         created_by)

# Get all fields from encounters
new_fungi = st_read(glue(qfield_path, "data.gpkg"), layer = fungi_layer)

# Get all fields from encounters
new_trees = st_read(glue(qfield_path, "data.gpkg"), layer = trees_layer)

# Get all fields from encounters
new_plants = st_read(glue(qfield_path, "data.gpkg"), layer = plants_layer)

#============================================================================
# Process the track data
#============================================================================

# Links
# https://github.com/r-spatial/sf/issues/692
# https://gis.stackexchange.com/questions/429144/is-there-a-way-to-select-nearest-feature-to-a-point-by-selected-column

# AFTER VETTING TRACKS. CAN DITCH START AND END TIMES? No. I will NOT always have a track. 
# Sometimes a survey will just be a spot check.

# Get all fields from track
track = st_read(glue(qfield_path, "data.gpkg"), layer = track_layer, crs = 4326)

# Extract parts. Need to explicitly set crs to be consistent with other geography
# Need to check why crs is displaying as 4326, but is actually set to 4326
track = track |> 
  filter(!is.na(survey_id)) |> 
  mutate(elevation = as.numeric(st_coordinates(geom)[,3])) |> 
  mutate(point_direction = if_else(point_direction == "nan", 
                                   NA_real_, as.numeric(point_direction))) |> 
  #st_zm() |> 
  select(survey_id, point_timestamp, horiz_accuracy, vert_accuracy,  
         elevation, point_groundspeed, point_direction, geometry = geom)

# Check...why does track import as 3857?
track$geometry[1]
st_crs(track)$epsg
st_crs(encounters)$epsg
class(track)

# Only add elevation and vert_accuracy if there was a track for the survey
if ( nrow(track) > 0L ) {
  # Add elevation from track table to features in encounters table
  encounters = encounters |> 
    mutate(elev_idx = st_nearest_feature(encounters, track))
  
  # Separate section to pull out data. Error when doing it in same block
  encounters = encounters |> 
    mutate(elevation = track$elevation[encounters$elev_idx]) |> 
    mutate(vert_accuracy = track$vert_accuracy[encounters$elev_idx])
  
  # Dump any data from elevation and vert_accuracy if survey_id in encounters is not present in track
  track_sid = tibble(survey_id = unique(track$survey_id),
                     track_present = "yes")
  encounters = encounters |> 
    left_join(track_sid, by = "survey_id") |> 
    mutate(elevation = if_else(track_present == "yes", elevation, NA_real_)) |> 
    mutate(vert_accuracy = as.numeric(vert_accuracy)) |> 
    mutate(vert_accuracy = if_else(track_present == "yes", vert_accuracy, NA_real_))
} else {
  encounters = encounters |> 
    mutate(elevation = NA_real_) |> 
    mutate(vert_accuracy = NA_real_)
}
  
# Convert track to linestring now that we have elevation in encounters (from track points)
trail = track |>
  filter(!is.na(survey_id)) |> 
  select(survey_id, elevation, geometry) |> 
  group_by(survey_id) |> 
  summarise(mn_elevation = mean(elevation), do_union = FALSE) |> 
  st_cast("MULTILINESTRING") |> 
  ungroup() |>
  select(survey_id, geometry)

# Add a location id and location_boundary_id
trail = trail |> 
  mutate(location_id = get_uuid(nrow(trail))) |> 
  mutate(location_route_id = get_uuid(nrow(trail))) |> 
  select(location_route_id, location_id, survey_id, geometry)

# Add created_by from survey
s_create = survey |> 
  st_drop_geometry() |> 
  select(survey_id, created_by)

trail = trail |> 
  left_join(s_create, by = "survey_id") |> 
  select(location_route_id, location_id, survey_id, created_by, geometry)

# Check
trail$geometry[1]
st_is_valid(trail)
#qtm(trail$geometry)

# # Does not work. Need to work on.
# m = leaflet(trail$geometry) |> 
#   addPolylines()
# m

#=========================================
# Combine species data
#=========================================

# Fungi
names(new_fungi) = c("fid", "species_id", "common_name", "genus",
                     "species", "long_name")
# Add species_type for fungi
new_fungi = new_fungi |> 
  mutate(species_type_id = "5808f34b-69e2-4b79-b624-4d88ee903f33")

# Trees
names(new_trees) = c("fid", "species_id", "common_name", "genus",
                     "species", "long_name")
# Add species_type for trees
new_trees = new_trees |> 
  mutate(species_type_id = "f8f010c1-634c-493b-be64-867954993ce7")

# Plants
names(new_plants) = c("fid", "species_id", "common_name", "genus",
                      "species", "long_name")
# Define all new plants as ground cover....can correct for few non-matches manually later
new_plants = new_plants |> 
  mutate(species_type_id = "b577ca21-3a0c-4aa7-ac9f-2643dd42dab1")

# Combine into one dataset. This will include all current species in DB
new_species = rbind(new_fungi, new_trees, new_plants)

#============================================================================
# Identify any new persons entered to the person table in qfield
# These need to be uploaded to myco DB
#============================================================================

# Get all survey_ids in myco DB
get_new_person = function(new_person) {
  # Pull out all person IDs from myco DB
  con = pg_con_local("myco")
  qry = glue("SELECT person_id ",
             "FROM person")
  prev_pers = DBI::dbGetQuery(con, qry)
  DBI::dbDisconnect(con)
  new_pers_entries = new_person |> 
    anti_join(prev_pers, by = "person_id")
  return(new_pers_entries)
}

# Define the insert new companions function
new_person_insert = function(con, new_pers_row) {
  person_id = new_pers_row$person_id
  first_name = new_pers_row$first_name
  last_name = new_pers_row$last_name
  created_by = new_pers_row$created_by
  insert_result = dbSendStatement(
    con, glue_sql("INSERT INTO person (",
                  "person_id, ",
                  "first_name, ",
                  "last_name, ",
                  "created_by) ",
                  "VALUES (",
                  "$1, $2, $3, $4)"))
  dbBind(insert_result, list(person_id, first_name, 
                             last_name, created_by))
  dbGetRowsAffected(insert_result)
  dbClearResult(insert_result)
}

#======================================================================
# Insert any new data entered to person table in qfield to myco DB

# Get the new companions entries
new_pers_entries = get_new_person(new_person)

# Run the insert function
if ( nrow(new_pers_entries) > 0L ) {
  con = pg_con_local("myco")
  for ( i in 1:nrow(new_pers_entries) ) {
    new_pers_row = new_pers_entries[i,]
    new_person_insert(con, new_pers_row)
  }
  DBI::dbDisconnect(con)
}

#============================================================================
# Identify any new species entered to the fungi, trees, or plants tables in
# qfield. These need to be uploaded to the myco DB
#============================================================================

# Get all survey_ids in myco DB
get_new_species = function(new_species) {
  # Pull out all companion IDs from myco DB
  con = pg_con_local("myco")
  qry = glue("SELECT species_id ",
             "FROM species_lut")
  prev_specs = DBI::dbGetQuery(con, qry)
  DBI::dbDisconnect(con)
  new_spec_entries = new_species |> 
    anti_join(prev_specs, by = "species_id")
  return(new_spec_entries)
}

# Define the insert new species function
new_species_insert = function(con, new_spec_row) {
  species_id = new_spec_row$species_id
  species_type_id = new_spec_row$species_type_id
  common_name = new_spec_row$common_name
  genus = new_spec_row$genus
  species = new_spec_row$species
  obsolete_flag = 0L
  insert_result = dbSendStatement(
    con, glue_sql("INSERT INTO species_lut (",
                  "species_id, ",
                  "species_type_id, ",
                  "common_name, ",
                  "genus, ",
                  "species, ",
                  "obsolete_flag) ",
                  "VALUES (",
                  "$1, $2, $3, $4, $5, $6)"))
  dbBind(insert_result, list(species_id, species_type_id, 
                             common_name, genus, species,
                             obsolete_flag))
  dbGetRowsAffected(insert_result)
  dbClearResult(insert_result)
}

#==============================================================================
# Insert any new data entered to fungi, trees, or plants tables in qfield to DB

# Get the new species entries. Will show only new species that need to be entered
new_spec_entries = get_new_species(new_species)

# Run the insert function
if ( nrow(new_spec_entries) > 0L ) {
  con = pg_con_local("myco")
  for ( i in 1:nrow(new_spec_entries) ) {
    new_spec_row = new_spec_entries[i,]
    new_species_insert(con, new_spec_row)
  }
  DBI::dbDisconnect(con)
}

#============================================================================
# Dump any data previously uploaded. For now, do not allow edits on the phone
# to be added to current survey or encounters data in DB
#============================================================================

# Get all survey_ids in myco DB
get_new_surveys = function(survey) {
  # Pull out data
  survey_ids = survey$survey_id
  s_ids = paste0(paste0("'", survey_ids, "'"), collapse = ", ")
  con = pg_con_local("myco")
  qry = glue("SELECT survey_id ",
             "FROM survey ",
             "WHERE survey_id in ({s_ids})")
  prev_sids = DBI::dbGetQuery(con, qry)
  DBI::dbDisconnect(con)
  new_surveys = survey |> 
    anti_join(prev_sids, by = "survey_id")
  return(new_surveys)
}

#========================================================
# Insert all data from survey
#========================================================

# Define survey as only data with new survey IDs
new_surveys = get_new_surveys(survey)

# Get rid of one test survey
no_surv_id = new_surveys$survey_id[new_surveys$start_point_name == "Home"]
new_surveys = new_surveys |> 
  filter(!survey_id %in% no_surv_id)
encounters = encounters |> 
  filter(!survey_id %in% no_surv_id)

#===================================
# Insert survey data (one row only)

# Run the function
for ( i in 1:nrow(new_surveys) ) {
  survey_row = new_surveys[i,]
  survey_insert(survey_row)
}

#=====================================
# Insert target species (one row only)

# Run the function
for ( i in 1:nrow(new_surveys) ) {
  survey_row = new_surveys[i,]
  target_species_insert(survey_row)
}

#=======================================
# Insert survey form data (one row only)

# Run the function
for ( i in 1:nrow(new_surveys) ) {
  survey_row = new_surveys[i,]
  survey_form_insert(survey_row)
}

#==========================================
# Insert survey device data (one row only)

# Run the function
for ( i in 1:nrow(new_surveys) ) {
  survey_row = new_surveys[i,]
  survey_device_insert(survey_row)
}

#===============================================
# Insert person data (may be multiple rows)

con = pg_con_local("myco")
for ( i in 1:nrow(new_surveys) ) {
  survey_row = new_surveys[i,]
  persons = person_rows(survey_row)
  # Run insert function on each row
  for ( j in 1:nrow(persons) ) {
    person_insert(con, persons[j,])
  }
}
DBI::dbDisconnect(con)

#==========================================
# Insert survey track data (one row only)

# Run the function
if ( nrow(trail) > 0L ) {
  for ( i in 1:nrow(trail) ) {
    track_row = trail[i,]
    survey_track_insert(track_row)
  }
}

#============================================================================
# Insert all data from encounters
#============================================================================

# Dump any encounter data with survey_ids not in new_surveys....TEST WITH REAL DATA !!!!!!
new_survey_ids = new_surveys |> 
  st_drop_geometry() |> 
  select(survey_id)
encounters = encounters |> 
  right_join(new_survey_ids, by = "survey_id") |> 
  filter(!is.na(fungi_type))

# Check survey_ids
# unique(encounters$survey_id)

# Insert encounters data
if ( nrow(encounters) > 0L ) {
  encounter_rows = encounter_data(encounters)
  prior_photos = get_all_photos(myco_photo_path)
  rename_qphotos(qmedia_path)
  media_rows = media_data(encounter_rows, prior_photos)
  copy_current_photos(media_rows, qmedia_path)
  nearby_rows = nearby_data(encounter_rows)
  
  # Load species encounter data if any exists
  con = pg_con_local("myco")
  DBI::dbWithTransaction(con, {
    if ( !is.null(encounter_rows) & nrow(encounter_rows) > 0L ) {
      for ( p in 1:nrow(encounter_rows) ) {
        species_encounter_insert(con, encounter_rows[p,])
      }
      # Load media location data if any exists
      if ( nrow(media_rows) > 0L ) {
        for ( q in 1:nrow(media_rows) ) {
          media_location_insert(con, media_rows[q,])
        }
      }
      # Load associated species encounter data if any exists
      if ( nrow(nearby_rows) > 0L ) {
        for ( r in 1:nrow(nearby_rows) ) {
          nearby_species_insert(con, associated_rows[r,])
        }
      }
    }
  })
  DBI::dbDisconnect(con)
}



